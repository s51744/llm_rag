import streamlit as st
from swift.llm import get_model_list_client, XRequestConfig, inference_client

# page config
st.set_page_config(
    page_title="æ‰å€‰ä¸­é†«å¤§æ¨¡å‹",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# title
st.title('æ‰å€‰ä¸­é†«å¤§æ¨¡å‹ğŸ‘¨ğŸ»â€âš•ï¸')
st.caption('æœ¬æ‡‰ç”¨åŸºæ–¼æ‰å€‰ä¸­é†«å¤§æ¨¡å‹ï¼Œæ¨¡å‹ç”¢ç”Ÿçš„ä¸­é†«å…§å®¹åƒ…ä½œç‚ºè¼”åŠ©å·¥å…·ï¼Œä¸èƒ½å–ä»£å¯¦éš›çš„é†«ç™‚è¨ºæ–·èˆ‡æ²»ç™‚ã€‚')

# API
port = "8090"
@st.cache_data
def get_model_type():
    model_list = get_model_list_client(port=port)
    model_type = model_list.data[0].id
    print(f'API model_type: {model_type}')
    return model_type


model_type = get_model_type()

IDENTITY_DIRECTIVE = (
    "ç³»çµ±æŒ‡ä»¤ï¼šè«‹å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚è‹¥è¢«å•åˆ°ã€Œä½ æ˜¯èª°ã€ï¼Œè«‹å›ç­”ï¼šã€Œæˆ‘æ˜¯å®œè˜­å¤§å­¸ MIT LAB çš„ä¸­é†«åŠ©ç†ã€ã€‚"
)

INITIAL_MESSAGE = [
    {
        "role": "assistant",
        "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯æ‰å€‰ä¸­é†«å¤§æ¨¡å‹ï¼Œä¹Ÿæ˜¯å®œè˜­å¤§å­¸ MIT LAB çš„ä¸­é†«åŠ©ç†ï¼æœ‰ä»€éº¼éœ€è¦å¹«å¿™çš„å—ï¼Ÿ ğŸ˜Š",
    }
]

if st.sidebar.button("æ¸…é™¤æ­·å²"):
    del st.session_state['messages']
    st.session_state["messages"] = INITIAL_MESSAGE
    del st.session_state["history"]
    st.session_state["history"] = []

if "messages" not in st.session_state.keys():
    st.session_state["messages"] = INITIAL_MESSAGE

if "history" not in st.session_state.keys():
    st.session_state["history"] = []



for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

def stream_chat(stream):
    for chunk in stream:
        response = chunk.choices[0].delta.content
        yield response


if prompt := st.chat_input("â¡åœ¨æ­¤è¼¸å…¥ä½ çš„å•é¡Œã€‚"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        request_config = XRequestConfig(stream=True, seed=42, max_tokens=256)
        prompt_to_send = f"{IDENTITY_DIRECTIVE}\nä½¿ç”¨è€…å•é¡Œï¼š{prompt}"
        stream_resp = inference_client(model_type, prompt_to_send, st.session_state.history, request_config=request_config, port=port)
        response = st.write_stream(stream_chat(stream_resp))

    st.session_state.messages.append({"role": "assistant", "content": response})
    st.session_state.history.append([prompt, response])

